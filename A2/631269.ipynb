{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6JVe8oNFmE9"
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o3OsknD23lFw"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "    roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.pyplot import subplots, show\n",
    "from functools import reduce\n",
    "from seaborn import lineplot\n",
    "from math import tanh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Util Functions\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "  return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def cross_entropy_error_function(target, y_hat):\n",
    "  return -np.sum( target * np.log(y_hat) + (1 - target) * np.log(1 - y_hat))\n",
    "\n",
    "def error_gradient(target, y_hat):\n",
    "  return -np.divide(target, y_hat) + np.divide(1 - target, 1 - y_hat)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "  return 1 - tanh(x) ** 2\n",
    "\n",
    "\n",
    "# Neural Network Implementation\n",
    "class neural_network():\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.A0 = self.A1 = self.A2 = self.Z1 = self.Z2 = self.W1 = self.W2 = 0\n",
    "\n",
    "    # Training Function\n",
    "    def train(self, data, epochs = 120, lr = 0.005):\n",
    "        W1 = np.random.random((1, 3))\n",
    "        W2 = np.random.random((1, 4))\n",
    "        error = []\n",
    "        for epoch in range(epochs):\n",
    "            error_row = np.zeros(len(data))\n",
    "            for idx, row in enumerate(data):\n",
    "                x_i = row[:-1]\n",
    "                y_i = row[-1]\n",
    "                # Forward Pass\n",
    "                A0 = np.append(x_i[:2], 1).reshape(1, -1).T\n",
    "                Z1 = W1 @ A0\n",
    "                A1 = tanh(Z1)\n",
    "                A1 = reduce(lambda x, y: np.append(x, y),\n",
    "                            [A1, x_i[2:], 1]).reshape(1, -1).T\n",
    "                Z2 = W2 @ A1\n",
    "                A2 = sigmoid(Z2).reshape(1, -1).T\n",
    "                # Backward Pass\n",
    "                dA2 = error_gradient(y_i, A2)\n",
    "                dZ2 = dA2 * sigmoid_derivative(Z2)\n",
    "                dW2 = dZ2[0] @ A1.T\n",
    "                dA1 = W2[:, :-1].T @ dZ2\n",
    "                dZ1 = dA1 * tanh_derivative(Z1)\n",
    "                dW1 = dZ1[0] @ A0.T\n",
    "                # Weight Update\n",
    "                W1 -= lr * dW1\n",
    "                W2 -= lr * dW2\n",
    "                error_row[idx] = cross_entropy_error_function(y_i, A2)\n",
    "            # Error\n",
    "            error.append(np.mean(error_row))\n",
    "        self.A0, self.A1, self.A2 = A0, A1, A2\n",
    "        self.Z1, self.Z2 = Z1, Z2\n",
    "        self.W1, self.W2 = W1, W2\n",
    "        return error\n",
    "\n",
    "    # Predicting Function\n",
    "    def predict(self, data):\n",
    "        A0, A1, A2 = self.A0, self.A1, self.A2\n",
    "        Z1, Z2 = self.Z1, self.Z2\n",
    "        W1, W2 = self.W1, self.W2\n",
    "        pred = []\n",
    "        for x_i in data:\n",
    "            A0 = np.append(x_i[:2], 1).reshape(1, -1).T\n",
    "            Z1 = W1 @ A0\n",
    "            A1 = tanh(Z1)\n",
    "            A1 = reduce(lambda x, y: np.append(x, y),\n",
    "                        [A1, x_i[2:], 1]).reshape(1, -1).T\n",
    "            Z2 = W2 @ A1\n",
    "            A2 = sigmoid(Z2).reshape(1, -1).T\n",
    "            pred.append(round(A2[0,0]))\n",
    "        return np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A155M4P47z3m"
   },
   "outputs": [],
   "source": [
    "# Loading and preparing data\n",
    "df = pd.read_csv(\"Part_1.tsv\", sep=\"\\t\")\n",
    "train, test = map(lambda x: np.array(x), train_test_split(df, test_size=.3))\n",
    "\n",
    "# Creating and Training Neural Network\n",
    "nn = neural_network()\n",
    "error = nn.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KCRfbVfm4DCr"
   },
   "outputs": [],
   "source": [
    "# Predicting test labels\n",
    "predicted = nn.predict(test[:,:-1])\n",
    "\n",
    "# Real labels\n",
    "actual = test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyPrlM8N4Fe-",
    "outputId": "78a0aef7-9a35-4c7c-be37-8bece4a9c777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      "╒════════════╤══════════╤═══════════╕\n",
      "│   accuracy │   auc_pr │   auc_roc │\n",
      "╞════════════╪══════════╪═══════════╡\n",
      "│   0.895667 │ 0.850332 │  0.894711 │\n",
      "╘════════════╧══════════╧═══════════╛\n",
      "\n",
      "Confusion Matrix:\n",
      "╒════════╤═══════╕\n",
      "│   1255 │   202 │\n",
      "╞════════╪═══════╡\n",
      "│    111 │  1432 │\n",
      "╘════════╧═══════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Calculating metrics\n",
    "metrics = {'accuracy': accuracy_score(actual, predicted),\n",
    "           'auc_pr': average_precision_score(actual, predicted),\n",
    "           'auc_roc': roc_auc_score(actual, predicted)}\n",
    "cm = confusion_matrix(actual, predicted)\n",
    "\n",
    "\n",
    "# Printing results\n",
    "print(\"Metrics:\")\n",
    "print(tabulate([metrics.keys(), metrics.values()],\n",
    "               headers='firstrow', tablefmt='fancy_grid'))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(tabulate(cm, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "f__vnGy-7ktv",
    "outputId": "da4c381c-080f-48af-8cc0-9dcd49f162c5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting error by Epoch\n",
    "fig, ax = subplots()\n",
    "lineplot(x=range(len(error)), y=error, ax=ax, label='Error')\n",
    "ax.set_xlabel('Epoch')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChfMdpQ79oX-"
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "02MOOtyFq45R"
   },
   "outputs": [],
   "source": [
    "# Implementation of bagged trees model\n",
    "# (some functions will also be used in random trees model)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data_prep(filename, delimiter=',', data_split=0.7, random_state=42):\n",
    "    \"\"\"Prepare the dataset to process, separating training and testing samples.\n",
    "    Imputs are the file to be importer, the delimiter, data split for training\n",
    "    set and random state. Returns train, test and data arrays\"\"\"\n",
    "    df = pd.read_csv(filename, delimiter = delimiter)\n",
    "    if df.iloc[:,-1].dtype != 'int64':\n",
    "        df_classes = df.iloc[:,-1].unique()\n",
    "        class_key = {}\n",
    "        for i in range (0, len(df_classes)):\n",
    "            class_key[df_classes[i]] = i\n",
    "        # Change labels to ints\n",
    "        df_class = df.iloc[:,-1]\n",
    "        df_name = df_class.name\n",
    "        # df_length = len(df_class)\n",
    "        df_classified = df.replace({df_name:class_key})\n",
    "        train = df_classified.sample(\n",
    "            frac=data_split, random_state=random_state)\n",
    "        test = df_classified.drop(train.index)\n",
    "        data = df_classified.values\n",
    "        train = train.values\n",
    "        test = test.values\n",
    "    else:\n",
    "        train = df.sample(frac=data_split, random_state=random_state)\n",
    "        test = df.drop(train.index)\n",
    "        data = df.values\n",
    "        train = train.values\n",
    "        test = test.values\n",
    "    train[:,-1] = train[:,-1] - 1\n",
    "    test[:,-1] = test[:,-1] - 1\n",
    "    data[:,-1] = data[:,-1] - 1\n",
    "    return train, test, data\n",
    "\n",
    "def gini(node):\n",
    "    \"\"\"Calculate the gini impurity for a node. Aim is to minimize gini\n",
    "    impurity(gain function).\"\"\"\n",
    "    # Find the number of classifications in current node.\n",
    "    classifications = node[:,-1]\n",
    "    samples = classifications.size\n",
    "    unique, counts = np.unique(classifications, return_counts = True)\n",
    "    # calculate gini based on number of classes\n",
    "    gini = 1\n",
    "    for i in range (0, unique.size):\n",
    "        proportion =  counts[i] / samples\n",
    "        gini = gini - proportion * proportion\n",
    "    return gini\n",
    "\n",
    "def gain(values, cur_gini, attribute, split):\n",
    "    \"\"\"Calculate information gain for an attribute split at each level.\n",
    "    Inputs are the current subset of data, initial gini at parent node,\n",
    "    attribute to be split and split number.\"\"\"\n",
    "    i = attribute\n",
    "    samples = values[:,-1].size\n",
    "    left = values[values[:,i] < split, :]\n",
    "    right = values[values[:,i] >= split, :]\n",
    "    left_samples = left[:,-1].size\n",
    "    right_samples = right[1:,-1].size\n",
    "    # Calculate left and right side gini\n",
    "    left_gini = gini(left)\n",
    "    right_gini = gini(right)\n",
    "    # Calculate information gain at this split value.\n",
    "    gain = cur_gini - (left_samples/samples)*left_gini -\\\n",
    "           (right_samples/samples)*right_gini\n",
    "    return gain, left, right\n",
    "\n",
    "def leaf(node):\n",
    "    \"\"\"Return classification value for leaf node, when either maximum depth of\n",
    "    tree reached or node is suitably weighted to one class.\"\"\"\n",
    "    classes = node[:, -1].tolist()\n",
    "    return max(set(node[:,-1]), key = classes.count)\n",
    "\n",
    "def split(node):\n",
    "    \"\"\"Find the ideal split point by searching for the best information gain\n",
    "    of all attributes and their potential split values. If no gain improves,\n",
    "    node is split for leaf node creation as right side left at 0 samples.\"\"\"\n",
    "    cur_gini = gini(node)\n",
    "    best_gain = best_attr = best_split = 0\n",
    "    # Implement greedy, exhaustive search for best information gain\n",
    "    variables = len(node[0])\n",
    "    best_left = node\n",
    "    best_right = np.empty([0,variables])\n",
    "    # Seach through each unique value to find best division\n",
    "    for v in range(0, variables-1):\n",
    "        uniques = np.unique(node[:, v])\n",
    "        for row in uniques:\n",
    "            new_gain, left, right  = gain(node, cur_gini, v, row)\n",
    "            # Select the best gain, and associated attributes\n",
    "            if new_gain > best_gain:\n",
    "                best_gain, best_attr, best_split, best_left, best_right = \\\n",
    "                    new_gain, v, row, left, right\n",
    "    return best_attr, best_split, best_left, best_right\n",
    "\n",
    "def decision(tree, max_depth=10, min_size=0, depth=0):\n",
    "    \"\"\"Uses split and leaf functions to build a tree, using a root data set.\n",
    "    Will assign leaf nodes if either maximum depth or minimum samples are\n",
    "    reached. root node contains both current node data, as well as decision\n",
    "    rules to that point.\"\"\"\n",
    "    left, right = tree[\"left\"], tree[\"right\"]\n",
    "    # If tree is at max depth, assign most common member.\n",
    "    if depth >= max_depth:\n",
    "        tree['left'], tree['right'] = leaf(left), leaf(right)\n",
    "    # If continuing sampling\n",
    "    else:\n",
    "        # Left side child\n",
    "        # If minimum samples exist in current node, make it a leaf with max\n",
    "        # occuring value in samples.\n",
    "        if left[:, -1].size <= min_size:\n",
    "            tree['left'] = leaf(left)\n",
    "        # Else continue building tree.\n",
    "        else:\n",
    "            left_attr, left_split, left_left, left_right = split(left)\n",
    "            # Check if node is terminal. Make it a leaf node if so.\n",
    "            if left_left.size == 0 or left_right.size == 0:\n",
    "                tree['left'] = leaf(np.vstack([left_left,left_right]))   \n",
    "            # Continue elsewise.\n",
    "            else:\n",
    "                tree['left'] = {\"attribute\": left_attr, \"split\": left_split,\n",
    "                                \"left\": left_left, \"right\": left_right,\n",
    "                                \"current_mode\": leaf(left)}\n",
    "                decision(tree['left'], max_depth, min_size, depth+1)\n",
    "        # right side child. Same process as above.\n",
    "        if right[:, -1].size <= min_size:\n",
    "            tree['right'] = leaf(right)\n",
    "        else:\n",
    "            right_attr, right_split, right_left, right_right = split(right)\n",
    "            if right_left.size == 0 or right_right.size == 0:\n",
    "                tree['right'] = leaf(np.vstack([right_left,right_right]))\n",
    "            else:\n",
    "                tree['right'] = {\"attribute\": right_attr, \"split\": right_split,\n",
    "                                 \"left\": right_left, \"right\": right_right,\n",
    "                                 \"current_mode\": leaf(right)}\n",
    "                decision(tree['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "\n",
    "def decision_tree_train(data, max_depth, min_size = 1):\n",
    "    \"\"\"Build decision tree. data refers to the training dataset.\n",
    "    max_depth refers to how deep the tree can get. min_size is the minimum\n",
    "    amount of samples before a leaf node must be classified.\"\"\"\n",
    "    max_depth = int(max_depth)\n",
    "    min_size = int(min_size)\n",
    "    attr, split_val, left, right = split(data)\n",
    "    tree = {\"attribute\": attr, \"split\": split_val, \"left\": left, \"right\": right,\n",
    "            \"current_mode\": leaf(data)}\n",
    "    decision(tree,max_depth,min_size)\n",
    "    return tree\n",
    "\n",
    "\n",
    "def bagged_tree_train(data, max_depth, min_size=1, subsample_ratio=1, trees =1):\n",
    "    \"\"\"Bagged decision trees contain a user-specified number of decision trees.\n",
    "    Classification of a sample is done by using the mode of each of these\n",
    "    decision trees. subsample is a fraction of the total dataset to be used.\n",
    "    trees refers to the number of trees to use in \"forest\" of trees. By leaving\n",
    "    default values for subsample and trees, a single decision tree classifier\n",
    "    is created.\"\"\"\n",
    "    \n",
    "    # Create a series of trees using sampling with replacement.\n",
    "    size = data[:, -1].size\n",
    "    division = int(size * subsample_ratio)        \n",
    "    forest = []\n",
    "    for i in range (0,trees):\n",
    "        samples = data[np.random.choice(data.shape[0], division, replace=True)]\n",
    "        forest.append([])\n",
    "        forest[i] = decision_tree_train(samples, max_depth, min_size)\n",
    "    return forest\n",
    "\n",
    "def classify(tree, row):\n",
    "    \"\"\"classify new data based on current row. Involves searching through tree\n",
    "    based on the attributes of validation data. Will return classification\n",
    "    value once leaf of tree is reached.\"\"\"\n",
    "    # Look at each sample to classify. append to list of output values.\n",
    "    # Recursively search through branches until an append can be made.\n",
    "    if row[tree['attribute']] < tree['split']:\n",
    "        if isinstance(tree['left'],dict):\n",
    "            return classify(tree['left'], row)\n",
    "        else:\n",
    "            return tree['left']\n",
    "    else:\n",
    "        if isinstance(tree['right'],dict):\n",
    "            return classify(tree['right'], row)\n",
    "        else:\n",
    "            return tree['right']\n",
    "\n",
    "def decision_tree_predict(tree, data):\n",
    "    \"\"\"For every row in the validation data, a call to the classify function\n",
    "    is done, with results appended to prediction data.\"\"\"\n",
    "    predictions = []\n",
    "    for row in data:\n",
    "        pred = classify(tree, row)\n",
    "        predictions.append(int(pred))\n",
    "    return predictions\n",
    "\n",
    "def bagged_tree_probas(forest, data):\n",
    "    \"\"\"\"Gets probabilites of labels based on built bagged trees. This is done\n",
    "    by taking the mode of the classifications of each decision tree.\"\"\"\n",
    "    forest_size = len(forest)\n",
    "    samples = len(data)\n",
    "    tree_classification = np.zeros((samples, forest_size))\n",
    "    # With each tree, find the classification of each validation sample.\n",
    "    for i in range (0, forest_size):\n",
    "        tree_classification[:, i] = decision_tree_predict(forest[i], data)\n",
    "    # Create list of modes for each sample, using tree_classification matrix.\n",
    "    predictions = []\n",
    "    for i in range(0, samples):\n",
    "        tree_pred = tree_classification[i,:].tolist()\n",
    "        predictions.append(tree_pred)\n",
    "    preds = [[t+1 for t in p] for p in predictions]\n",
    "    return np.array([sum(p)/len(p) for p in preds])\n",
    "\n",
    "def bagged_tree_predict(forest, data):\n",
    "    \"\"\"Get predictions of labels based on model and itrs probas\"\"\"\n",
    "    probas = bagged_tree_probas(forest, data)\n",
    "    return np.array([0 if p>.5 else -1 for p in probas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6OKXiDKZUXT"
   },
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nOSSdp3sW-0U"
   },
   "outputs": [],
   "source": [
    "# Implementation of Random Forest\n",
    "# Using trees already implemented on Part 2\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "\n",
    "def bootstrap_sample(data, bootstrapSize):\n",
    "    \"\"\"\"Get bootstrap sample with given size\"\"\"\n",
    "    randomIndices = numpy.random.randint(\n",
    "        low=0, high=len(data), size=bootstrapSize)\n",
    "    return data[randomIndices]\n",
    "\n",
    "def create_random_forest(\n",
    "        data, bootstrapSize, forestSize=20, max_depth=20, min_size=1):\n",
    "    \"\"\"Vreates Random Forest based on the trees already implemented. Given the\n",
    "    data, the bootstrap samples size, forest size, maximum deepth and minimum\n",
    "    size of the trees, returns a list of trees (a forest)\"\"\"\n",
    "    forest = []\n",
    "    for i in range(forestSize):\n",
    "        sample = bootstrap_sample(data, bootstrapSize)\n",
    "        decisionTree = decision_tree_train(sample, max_depth, min_size)\n",
    "        forest.append(decisionTree)\n",
    "    return forest\n",
    "\n",
    "def random_forest_probas(data, randomForest):\n",
    "    \"\"\"Predict labels probabilities based on singular trees predictions. Given\n",
    "    data to predict and a random forest, returns a vector of probabilities\"\"\"\n",
    "    predictions = {}\n",
    "    for i in range(len(randomForest)):\n",
    "        column = \"decision tree \" + str(i)\n",
    "        predictions[column] = decision_tree_predict(randomForest[i], data)\n",
    "    predictions = pandas.DataFrame(predictions)\n",
    "    return (predictions.shape[1]+predictions.sum(axis=1))/predictions.shape[1]\n",
    "\n",
    "def random_rorest_predictions(data, randomForest):\n",
    "    \"\"\"Predicts label based on mode of trees predictions. Given data to predict\n",
    "    and random forest, returns a vector of predictions\"\"\"\n",
    "    probas = random_forest_probas(data, randomForest)\n",
    "    return np.array([0 if p>.5 else -1 for p in probas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-PRLE2OZFsH"
   },
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eCCr60vr1WQ-"
   },
   "outputs": [],
   "source": [
    "train, test, data = data_prep('adult.data')\n",
    "\n",
    "bagged_tree_model = bagged_tree_train(\n",
    "    train, max_depth=3, min_size=1, subsample_ratio=1, trees=3)\n",
    "\n",
    "random_forest_model = create_random_forest(\n",
    "    train, 100, forestSize=10, max_depth=3, min_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uzPq8gb8qV61"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def metrics(actual, predicted, probas):\n",
    "    \"\"\"Return a confusion matrix showing the difference between actual values,\n",
    "    and model predicted values. Also returns total accuracy\"\"\"\n",
    "    actual = actual.astype(int)\n",
    "    classes = np.unique(actual)\n",
    "    matrix = np.zeros((len(classes), len(classes)))\n",
    "    for a, p in zip(actual, predicted):\n",
    "        matrix[a][p] += 1\n",
    "    precision = matrix[0,0] / sum(matrix[:,0])\n",
    "    recall = matrix[0,0] / sum(matrix[0,:])\n",
    "    auc_roc = roc_auc_score(actual+1, probas)\n",
    "    auc_pr = average_precision_score(actual+1, probas)\n",
    "    return {'precision': precision,\n",
    "            'recall': recall,\n",
    "            'auc_roc': auc_roc,\n",
    "            'auc_pr': auc_pr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWi5ZztJwXQ6",
    "outputId": "a3095fe0-ef02-4eb4-d604-93d411c0dbee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤═════════════╤══════════╤═══════════╤══════════╕\n",
      "│               │   precision │   recall │   auc_roc │   auc_pr │\n",
      "╞═══════════════╪═════════════╪══════════╪═══════════╪══════════╡\n",
      "│ bagged trees  │    0.766958 │ 0.494158 │  0.736501 │ 0.519546 │\n",
      "├───────────────┼─────────────┼──────────┼───────────┼──────────┤\n",
      "│ random forest │    0.792939 │ 0.359585 │  0.849005 │ 0.650802 │\n",
      "╘═══════════════╧═════════════╧══════════╧═══════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "X, Y = test[:,:14], test[:, -1]\n",
    "\n",
    "bt_probas = bagged_tree_probas(bagged_tree_model, X)\n",
    "rf_probas = random_forest_probas(X, random_forest_model)\n",
    "\n",
    "bt_pred = bagged_tree_predict(bagged_tree_model, X)\n",
    "rf_pred = random_rorest_predictions(X, random_forest_model)\n",
    "\n",
    "print(tabulate(\n",
    "    [['']+list(metrics(Y, bt_pred, bt_probas).keys()),\n",
    "     ['bagged trees']+list(metrics(Y, bt_pred, bt_probas).values()),\n",
    "     ['random forest']+list(metrics(Y, rf_pred, rf_probas).values())],\n",
    "    headers='firstrow', tablefmt='fancy_grid'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "631269.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
